2018-05-03
===========

Experiment
----------
### 05:08:19 PM

We try to run the gmake benchmark with hugepages enabled on both l1 and l0.

We setup hugepages in both l0 and l1 following this guide:
https://help.ubuntu.com/community/KVM%20-%20Using%20Hugepages

Each machine has hugepages set to about half it's available memory, and
slightly more than the memory required by l(N+1) VM.

We make 3 modifications:
1. Enable hugepages on the host machine (relative host to each level)
2. edit /etc/sysctl.conf to allow for a sufficent number of pages
3. edit /etc/default/qemu-kvm to make qemu use the hugepages, by setting `KVM_HUGEPAGES=1`.
4. reboot

Once these modifications are made on both l0 and l1 (respectively, l1 must be booted and modified *after* l0 has the required hugepages enabled), we can run our experiments.

Warning: It's very easy to screw up /etc/sysctl.conf and lock yourself out of your VM. Fixing the disk is a difficult/non-trivial task. But one can follow the guide here: https://docs.openstack.org/image-guide/modify-images.html ; The qemu-nbd method is the one that worked for me.

Tip: VMs did not have easy access to a text serial console, but an easy way to
gain access to a remote VNC bound to "localhost" on the host is to use this following:
```
# Get access to vnc for l1 vm:
# Assuming vnc is running on 127.0.0.1:0 on l0 host
remote$ ssh -L 6999:localhost:5900 <l0-host>
remote$ vncviewer localhost:6999

# get access to vnc for l2 vm:
# Assuming vnc is running on 127.0.0.1:0 on l1 host
remote$ ssh -L 6998:localhost:6998 <l0-host>
l0-host$ ssh -L 6998:localhost:5900 <l1-host>
remote$ vncviewer localhost:6998
```

Conclusion:
There is no notable difference in the perf results with and without hugepages. There is a fractional difference of `<1%`, some of which can probably be attributed to additional automation between the first and second run.

With hugepages:
```
-   70.40%    69.85%  qemu-system-x86  [kernel.kallsyms]   [k] native_queued_spin_lock_slowpath
   + 66.34% entry_SYSCALL_64_fastpath
   + 3.51% 0xffffffffffffffff
     0.55% native_queued_spin_lock_slowpath
```

without hugepages:
```
-   70.91%    70.71%  qemu-system-x86  [kernel.kallsyms]        [k] native_queued_spin_lock_slowpath
    + 66.22% entry_SYSCALL_64_fastpath
    + 4.49% 0xffffffffffffffff
```


2018-05-05
==========

Experiment
----------

### 06:33:34 AM
Compile kernel with CST/CST-MCS lock implementation and run perf again.


Outcome: Looks like we see reduced contention down from ~70% to 45%, confirmed with perf.

Should re-run the full benchmarks and compare the throughput as well.

